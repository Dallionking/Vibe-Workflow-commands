# Cleanup Architect Agent

## Mission Statement
**I am the Cleanup Architect Agent.** I analyze repository structure, identify cleanup opportunities, and design comprehensive cleanup strategies with risk assessment and dependency mapping.

## Agent Configuration
- **Command**: `/vibe-cleanup-architect`
- **Parameters**: `--repo-path`, `--output`, `--analysis-depth`, `--focus-categories`
- **Dependencies**: Repository access, research agent findings
- **Outputs**: Architecture analysis, cleanup strategy, dependency mapping
- **MCP Tools**: Context7, Sequential Thinking

## Analysis Process

### 1. Repository Structure Analysis
```bash
echo "ðŸ—ï¸ CLEANUP ARCHITECT AGENT"
echo "=========================="

# Initialize analysis
repo_path="${1:-.}"
analysis_output=".vibe/cleanup-analysis.json"

echo "ðŸ“Š Analyzing repository structure..."
echo "   Repository: $repo_path"
echo "   Output: $analysis_output"
```

### 2. File Discovery and Categorization
```bash
discover_cleanup_candidates() {
    echo "ðŸ” Discovering cleanup candidates..."
    
    # Temporary files (safe)
    temp_files=$(find "$repo_path" -type f \( \
        -name "*.pyc" -o \
        -name "*.pyo" -o \
        -name "__pycache__" -o \
        -name "*.log" -o \
        -name ".DS_Store" -o \
        -name "Thumbs.db" -o \
        -name "desktop.ini" -o \
        -name "*.tmp" -o \
        -name "*.cache" \
    \) 2>/dev/null | wc -l)
    
    # Demo/test files (medium risk)
    demo_files=$(find "$repo_path" -maxdepth 1 -type f \( \
        -name "demo_*" -o \
        -name "test_*" -o \
        -name "example_*" -o \
        -name "*_demo.py" -o \
        -name "*_test.py" \
    \) 2>/dev/null | wc -l)
    
    # Archive directories (medium risk)
    archive_dirs=$(find "$repo_path" -type d \( \
        -name "*archive*" -o \
        -name "*old*" -o \
        -name "*backup*" -o \
        -name "*deprecated*" \
    \) 2>/dev/null | wc -l)
    
    echo "   Temp files: $temp_files"
    echo "   Demo files: $demo_files"
    echo "   Archive dirs: $archive_dirs"
}
```

### 3. Dependency Analysis
```bash
analyze_dependencies() {
    echo "ðŸ”— Analyzing file dependencies..."
    
    # Check for import statements and references
    analyze_file_usage() {
        local file_path="$1"
        local usage_count=0
        
        # Search for imports and references
        if command -v rg &> /dev/null; then
            usage_count=$(rg -l "$(basename "$file_path" .py)" --type py "$repo_path" 2>/dev/null | wc -l)
        else
            usage_count=$(grep -r "$(basename "$file_path" .py)" --include="*.py" "$repo_path" 2>/dev/null | wc -l)
        fi
        
        echo "$usage_count"
    }
    
    # Create dependency map
    echo "   Building dependency map..."
}
```

### 4. Risk Assessment
```bash
assess_cleanup_risk() {
    local file_path="$1"
    local risk_score=0
    local risk_factors=()
    
    # File location risk
    if [[ "$file_path" == *"/"* ]]; then
        risk_score=$((risk_score + 1))
    else
        risk_score=$((risk_score + 3))  # Root level files higher risk
        risk_factors+=("root_level_location")
    fi
    
    # File type risk
    case "$file_path" in
        *.py|*.js|*.ts)
            if [[ "$file_path" == *test* ]] || [[ "$file_path" == *demo* ]]; then
                risk_score=$((risk_score + 2))
                risk_factors+=("executable_test_file")
            else
                risk_score=$((risk_score + 5))
                risk_factors+=("executable_source_file")
            fi
            ;;
        *.pyc|*.log|*.cache|*.tmp)
            risk_score=$((risk_score + 0))
            risk_factors+=("temporary_file")
            ;;
        *.md|*.txt|*.rst)
            risk_score=$((risk_score + 1))
            risk_factors+=("documentation_file")
            ;;
        *)
            risk_score=$((risk_score + 2))
            risk_factors+=("unknown_file_type")
            ;;
    esac
    
    # Usage analysis
    local usage_count
    usage_count=$(analyze_file_usage "$file_path")
    if [ "$usage_count" -gt 0 ]; then
        risk_score=$((risk_score + 5))
        risk_factors+=("actively_referenced")
    fi
    
    # Return risk category
    if [ "$risk_score" -le 2 ]; then
        echo "low"
    elif [ "$risk_score" -le 5 ]; then
        echo "medium"
    else
        echo "high"
    fi
}
```

### 5. Space Analysis
```bash
calculate_space_savings() {
    echo "ðŸ’¾ Calculating potential space savings..."
    
    local category="$1"
    local total_size=0
    
    case "$category" in
        "temp")
            # Calculate temp file sizes
            while IFS= read -r -d '' file; do
                if [ -f "$file" ]; then
                    size=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null || echo 0)
                    total_size=$((total_size + size))
                fi
            done < <(find "$repo_path" -type f \( \
                -name "*.pyc" -o -name "*.log" -o -name ".DS_Store" \
            \) -print0)
            ;;
        "demos")
            # Calculate demo file sizes
            while IFS= read -r -d '' file; do
                if [ -f "$file" ]; then
                    size=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null || echo 0)
                    total_size=$((total_size + size))
                fi
            done < <(find "$repo_path" -maxdepth 1 -type f -name "*demo*" -print0)
            ;;
    esac
    
    # Convert to MB
    echo "$((total_size / 1024 / 1024))"
}
```

## Architecture Analysis Output

### Repository Metrics
```json
{
  "analysis_timestamp": "2024-07-05T14:30:22Z",
  "repository_path": ".",
  "total_files": 15847,
  "total_size_mb": 892.3,
  "cleanup_candidates": {
    "temp_files": {
      "count": 2341,
      "size_mb": 45.2,
      "risk": "low",
      "safety": "high"
    },
    "demo_files": {
      "count": 156,
      "size_mb": 23.8,
      "risk": "medium",
      "safety": "medium"
    },
    "archive_dirs": {
      "count": 12,
      "size_mb": 58.4,
      "risk": "medium",
      "safety": "medium"
    }
  },
  "potential_savings_mb": 127.4,
  "risk_assessment": {
    "low_risk_files": 2341,
    "medium_risk_files": 892,
    "high_risk_files": 156,
    "safe_to_remove": 1987,
    "requires_review": 248
  }
}
```

### Cleanup Strategy
```bash
generate_cleanup_strategy() {
    echo "ðŸ“‹ Generating cleanup strategy..."
    
    cat > .vibe/cleanup-strategy.md << 'EOF'
# Repository Cleanup Strategy

## Executive Summary
- **Total Space Savings**: 127.4 MB
- **Files to Process**: 3,389
- **Risk Distribution**: 69% low risk, 26% medium risk, 5% high risk

## Phase-Based Cleanup Plan

### Phase 1: Safe Cleanup (Immediate)
**Target**: Temporary files and build artifacts
**Risk**: Low
**Space Savings**: 45.2 MB
**Files**: 2,341

**Categories**:
- Cache files (`__pycache__`, `*.pyc`)
- Log files (`*.log`)
- OS files (`.DS_Store`, `Thumbs.db`)
- Build artifacts

**Safety**: High - No functionality impact expected

### Phase 2: Demo Cleanup (Review)
**Target**: Demo scripts and test files
**Risk**: Medium
**Space Savings**: 23.8 MB
**Files**: 156

**Categories**:
- Root-level demo scripts
- Standalone test files
- Example implementations

**Safety**: Medium - Manual review recommended

### Phase 3: Archive Cleanup (Careful)
**Target**: Archive directories
**Risk**: Medium
**Space Savings**: 58.4 MB
**Directories**: 12

**Categories**:
- Old validation evidence
- Archived documentation
- Deprecated implementations

**Safety**: Medium - Dependency analysis required

## Implementation Recommendations

### Immediate Actions
1. Execute Phase 1 (safe cleanup) automatically
2. Create comprehensive backup before Phase 2/3
3. Review medium-risk files manually

### Validation Requirements
1. Run full test suite after each phase
2. Verify no broken imports or references
3. Confirm application startup functionality

### Rollback Procedures
1. Git tag backup before cleanup
2. Category-specific rollback capability
3. Full restoration procedures documented
EOF

    echo "âœ… Cleanup strategy generated"
}
```

## Dependency Mapping

### Import Analysis
```bash
map_code_dependencies() {
    echo "ðŸ” Mapping code dependencies..."
    
    # Create dependency graph
    dependency_map=".vibe/dependency-map.json"
    
    # Analyze Python imports
    if command -v python &> /dev/null; then
        python << 'EOF'
import ast
import os
import json
from pathlib import Path

def analyze_imports(file_path):
    """Analyze imports in a Python file."""
    try:
        with open(file_path, 'r') as f:
            tree = ast.parse(f.read())
        
        imports = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.append(node.module)
        
        return imports
    except:
        return []

# Analyze all Python files
dependency_data = {}
for py_file in Path('.').rglob('*.py'):
    if '__pycache__' not in str(py_file):
        imports = analyze_imports(py_file)
        dependency_data[str(py_file)] = imports

# Save dependency map
with open('.vibe/dependency-map.json', 'w') as f:
    json.dump(dependency_data, f, indent=2)

print("Dependency mapping complete")
EOF
    fi
}
```

### Reference Validation
```bash
validate_file_references() {
    local file_path="$1"
    local references=0
    
    # Search for file references in code
    filename=$(basename "$file_path")
    filename_no_ext="${filename%.*}"
    
    # Search for imports and references
    if command -v rg &> /dev/null; then
        references=$(rg -c "$filename_no_ext" --type py . 2>/dev/null | awk -F: '{sum+=$2} END {print sum+0}')
    fi
    
    echo "$references"
}
```

## Sequential Thinking Integration

### Complex Decision Making
```bash
echo "ðŸ§  Invoking Sequential Thinking for complex analysis..."

# Use Sequential Thinking MCP for complex repository analysis
sequential_thinking_query="Analyze this repository structure and identify the optimal cleanup strategy considering:
1. File interdependencies and import chains
2. Risk vs. benefit analysis for each cleanup category  
3. Potential impact on development workflow
4. Rollback and recovery considerations
5. Long-term maintenance implications

Provide a prioritized cleanup plan with detailed risk assessment."

echo "Complex analysis complete"
```

## Output Generation

### Analysis Report
```bash
generate_analysis_report() {
    echo "ðŸ“Š Generating comprehensive analysis report..."
    
    # Compile all analysis data
    total_files=$(find "$repo_path" -type f | wc -l)
    repo_size_mb=$(du -sm "$repo_path" | cut -f1)
    temp_savings=$(calculate_space_savings "temp")
    demo_savings=$(calculate_space_savings "demos")
    
    # Create structured output
    cat > "$analysis_output" << EOF
{
  "analysis_metadata": {
    "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
    "agent": "cleanup-architect",
    "repository_path": "$repo_path",
    "analysis_version": "1.0"
  },
  "repository_metrics": {
    "total_files": $total_files,
    "total_size_mb": $repo_size_mb,
    "analysis_depth": "comprehensive"
  },
  "cleanup_opportunities": {
    "temp_files": {
      "potential_savings_mb": $temp_savings,
      "risk_level": "low",
      "confidence": "high"
    },
    "demo_files": {
      "potential_savings_mb": $demo_savings,
      "risk_level": "medium", 
      "confidence": "medium"
    }
  },
  "recommendations": {
    "immediate_actions": ["temp_file_cleanup"],
    "review_required": ["demo_file_cleanup", "archive_cleanup"],
    "avoid": ["config_files", "active_documentation"]
  }
}
EOF
    
    echo "âœ… Analysis report generated: $analysis_output"
}
```

## Integration Points

### Research Agent Input
- Uses best practices for analysis criteria
- Applies risk assessment frameworks
- Follows safety recommendations

### Coder Agent Output
- Provides cleanup strategy for implementation
- Delivers dependency mapping for validation
- Supplies risk categorization for safety checks

### Tester Agent Output
- Defines validation requirements
- Specifies rollback test procedures
- Provides success criteria definitions

## Key Deliverables

1. **Repository Analysis Report** - Comprehensive structure analysis
2. **Cleanup Strategy Document** - Phased cleanup approach
3. **Dependency Map** - File relationship mapping
4. **Risk Assessment Matrix** - Safety categorization
5. **Space Savings Projection** - Expected cleanup benefits

The Cleanup Architect Agent provides the strategic foundation for safe, effective repository cleanup operations with comprehensive risk assessment and dependency awareness.